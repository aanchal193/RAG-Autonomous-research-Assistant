{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import arxiv\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if not exists\n",
    "dirpath = \"arxiv_papers\"\n",
    "if not os.path.exists(dirpath):\n",
    "    os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search arXiv for papers related to \"LLM\"\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    query=\"LLM\",\n",
    "    max_results=10,\n",
    "    sort_order=arxiv.SortOrder.Descending\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Paper id 2406.10300v1 with title 'Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications' is downloaded.\n",
      "-> Paper id 2405.19888v1 with title 'Parrot: Efficient Serving of LLM-based Applications with Semantic Variable' is downloaded.\n",
      "-> Paper id 2311.10372v2 with title 'A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends' is downloaded.\n",
      "-> Paper id 2404.14809v1 with title 'A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications' is downloaded.\n",
      "-> Paper id 2308.08241v2 with title 'TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series' is downloaded.\n",
      "-> Paper id 2408.02479v1 with title 'From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future' is downloaded.\n",
      "-> Paper id 2402.18050v1 with title 'MEGAnno+: A Human-LLM Collaborative Annotation System' is downloaded.\n",
      "-> Paper id 2402.08030v1 with title 'Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking' is downloaded.\n",
      "-> Paper id 2408.13006v1 with title 'Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates' is downloaded.\n",
      "-> Paper id 2307.09793v1 with title 'On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models' is downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Download and save the papers\n",
    "for result in client.results(search):\n",
    "    while True:\n",
    "        try:\n",
    "            result.download_pdf(dirpath=dirpath)\n",
    "            print(f\"-> Paper id {result.get_short_id()} with title '{result.title}' is downloaded.\")\n",
    "            break\n",
    "        except (FileNotFoundError, ConnectionResetError) as e:\n",
    "            print(\"Error occurred:\", e)\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pages loaded: 239\n"
     ]
    }
   ],
   "source": [
    "# Load papers from the directory\n",
    "papers = []\n",
    "loader = DirectoryLoader(dirpath, glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "try:\n",
    "    papers = loader.load()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "print(\"Total number of pages loaded:\", len(papers)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all pages' content into a single string\n",
    "full_text = ''\n",
    "for paper in papers:\n",
    "    full_text += paper.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in the concatenated text: 1056041\n"
     ]
    }
   ],
   "source": [
    "# Remove empty lines and join lines into a single string\n",
    "full_text = \" \".join(line for line in full_text.splitlines() if line)\n",
    "print(\"Total characters in the concatenated text:\", len(full_text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "paper_chunks = text_splitter.create_documents([full_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(\"localhost\", port=6333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Qdrant vector store\n",
    "qdrant = Qdrant.from_documents(\n",
    "    documents=paper_chunks,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    path=\"./tmp/local_qdrant\",\n",
    "    collection_name=\"arxiv_papers\",\n",
    ")\n",
    "retriever = qdrant.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aanch\\AppData\\Local\\Temp\\ipykernel_3668\\2757140565.py:3: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  model = ChatOllama(model=ollama_llm)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama LLM\n",
    "ollama_llm = \"llama2:7b-chat\"\n",
    "model = ChatOllama(model=ollama_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the processing chain\n",
    "chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add typing for input\n",
    "class Question(BaseModel):\n",
    "    __root__: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, Vision Enhancing LLMs (VELLMs) are a type of Language Model that can enhance or augment visual input with text-based content. The main difference between VELLMs and traditional LLMs is their ability to integrate with other systems and modalities, such as visual input or real-world perception data, enabling them to perform more complex and context-based decision-making tasks.\n",
      "\n",
      "VELLMs are designed to process and generate text-based content in multi-modal settings, where they can interact with visual inputs or real-world data to improve their performance. This allows VELLMs to perform tasks that traditional LLMs cannot, such as understanding the context of visual inputs or generating responses based on both textual and visual inputs.\n",
      "\n",
      "The key insight of this research is that rather than analyzing an LLM-integrated application in whole, analysis should start with the identification and description of its distinct LLM components. This allows for a clearer understanding of how the application utilizes the capabilities of LLMs, as the LLM-integrated application manifests as a combination of its LLM components.\n",
      "\n",
      "In terms of learning and adaptation mechanisms, VELLMs have the ability to adapt through new data updates, but they lack the ability to continuously learn from real-time feedback. Instead, they rely on using existing knowledge to solve problems and generate responses.\n",
      "\n",
      "Overall, VELLMs represent a significant advancement in the field of Language Models, enabling them to interact with other systems and modalities in more sophisticated ways, leading to improved performance and decision-making capabilities.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply input type to the chain\n",
    "chain = chain.with_types(input_type=Question)\n",
    "result = chain.invoke(\"Explain about Vision Enhancing LLMs\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
